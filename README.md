# Lokal Excel LLM App

Detta projekt tillhandahÃ¥ller en **sjÃ¤lvbÃ¤rande Streamlit-applikation** som lÃ¥ter anvÃ¤ndare stÃ¤lla frÃ¥gor Ã¶ver en eller flera Excel-filer med hjÃ¤lp av **lokala LLM-modeller via Ollama**.

Projektet Ã¤r designat fÃ¶r att fungera **plattformoberoende**:

* Linux (CPU, AMD GPU via ROCm/MESA)
* Windows 11 (CPU eller NVIDIA GPU via Docker Desktop)
* macOS (CPU)

Samma Docker-image anvÃ¤nds i alla fall â€“ skillnaden ligger endast i **hur containern startas**.

---

## Arkitektur â€“ Ã¶versikt

* **Streamlit** â€“ webbgrÃ¤nssnitt (`http://localhost:8501`)
* **Ollama** â€“ lokal LLM-server (CPU/GPU automatiskt)
* **ChromaDB** â€“ lokal vektordatabas
* **Docker** â€“ distribution och isolering

GPU-stÃ¶d aktiveras **vid runtime**, inte i koden.

---

## Projektstruktur

```
excel-llm-app/
â”œâ”€â”€ app/
â”‚   â””â”€â”€ query_llm.py
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ entrypoint.sh
â”‚   â”œâ”€â”€ compose.cpu.yml
â”‚   â”œâ”€â”€ compose.nvidia.yml
â”‚   â””â”€â”€ compose.amd.yml
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## Hur man bygger docker-imagen

Byggs en gÃ¥ng (pÃ¥ Linux eller Windows):

```bash
docker build -t excel-llm-app -f docker/Dockerfile .
```

---

## Starta applikationen

### CPU-only (alla plattformar)

```bash
docker compose -f docker/compose.cpu.yml up
```

### NVIDIA GPU (Windows eller Linux)

Krav:

* NVIDIA GPU
* NVIDIA-drivrutin
* Docker Desktop med GPU-stÃ¶d

```bash
docker compose -f docker/compose.nvidia.yml up
```

Alternativt:

```bash
docker run --gpus all -p 8501:8501 -v ollama-data:/root/.ollama excel-llm-app
```

### AMD GPU (Linux / NixOS)

Krav:

* AMD GPU
* ROCm/MESA installerat pÃ¥ host

```bash
docker compose -f docker/compose.amd.yml up
```

MD GPU stÃ¶ds **inte** pÃ¥ Windows i nulÃ¤get â€“ dÃ¤r anvÃ¤nds CPU fallback.

---

## AnvÃ¤ndning

1. Starta containern (enligt ovan)
2. Ã–ppna webblÃ¤sare:

```
http://localhost:8501
```

3. Ladda upp en eller flera `.xlsx`-filer
4. VÃ¤lj sprÃ¥kmodell (endast installerade Ollama-modeller visas)
5. StÃ¤ll frÃ¥gor Ã¶ver samtliga tabeller

---

## Modeller ("thin image")

Docker-imagen innehÃ¥ller **inga LLM-modeller** i sin fÃ¶rsta start.

* Modeller laddas automatiskt via Ollama vid fÃ¶rsta anvÃ¤ndning
* Laddade modeller sparas i Docker-volymen:

```
ollama-data â†’ /root/.ollama
```

Ã¤sta start Ã¤r omedelbar.

---

## Vanliga frÃ¥gor

### BehÃ¶ver anvÃ¤ndaren installera Python eller Ollama?

Nej â€“ allt kÃ¶rs i Docker.

### Fungerar detta offline?

Ja, efter att modeller laddats fÃ¶rsta gÃ¥ngen.

### Hur stor Ã¤r imagen?

* Basimage: ~1â€“2 GB
* Modeller: 2â€“40+ GB beroende pÃ¥ val

---

## BegrÃ¤nsningar

* AMD GPU stÃ¶ds endast pÃ¥ Linux
* Windows + AMD â†’ CPU-only
* Mycket stora modeller krÃ¤ver mycket RAM/VRAM

---

## Licenser

Detta projekt Ã¤r Apache-2 men avsett fÃ¶r intern anvÃ¤ndning, experiment och forskning.
Respektera respektive modells licensvillkor (Gemma, LLaMA, m.fl.).

---

## Rekommenderad vidareutveckling

* Multi-user sessioner
* Persistens av ChromaDB

---

**Bygg en image. KÃ¶r den Ã¶verallt. LÃ¥t Ollama gÃ¶ra resten.** ğŸš€
